---
title: 大模型不是黑洞--《图解大模型：生成式AI原理与实战》
bookname: 《图解大模型：生成式AI原理与实战》
date: 2025-08-23 13:42:17
tags:
categories: 读书笔记
---

# 序

昨天终于读完了《图解大模型：生成式AI原理与实战》，合上书，长舒一口气，轻声感叹：其实，大模型并没有那么神秘，也并非什么晦涩难懂的玩意儿。

这本书是近年来我读过的、在深度学习、Transformer架构以及大模型原理与技术方面讲得最清晰、最系统的一本。让我仿佛重回学生时代，读完原版《计算机体系结构》时那种豁然开朗的感觉。

在我看来，真正优秀的技术类图书，必须能够在“目有全牛”与“目无全牛”之间自如切换——既能从宏观上把握全貌，又能窥见关键细节。但它不应陷入细节的泥沼，而应让读者在阅读过程中产生乐趣，并在需要时，能按图索骥，深入探究细节的实现。

<!-- more -->

# 大模型不是黑洞--《图解大模型：生成式AI原理与实战》

这本书让我爱不释手，几乎是逐字逐句地读完的。除了部分代码因没有亲自跑一遍略显生涩之外，整体阅读体验非常愉快。

回想起来，我第一次接触中文分词、语义分析和知识发现是在2006年年底。可惜之后在这个方向几乎未曾延续，毕竟我后来在技术路径的选择上并未聚焦于人工智能。近年来，随着这类技术的迅猛发展，我也多次想要补上这些年在理论和实践上的短板，但总是难以真正坚持下去。直到2025年的年中，一次性补了20年课，后劲确实有点猛。

## 黄鼠狼

这些年我更多是在持续跟进主流的前端和后端技术演进，努力提升自己对软件工程以及手头项目的理解。毕竟，后者更偏重于工程实践，只要愿意花时间阅读文档、跟着教程一步步操作，常常能获得比预期更深刻的理解。

比如最近，我照着官方文档，搭建了一个基于一次性密码（OTP）的安全文件共享系统，也算是体验了一把新一代 Serverless 架构的开发流程。做完之后，还顺手发了条朋友圈感慨了一下：

> 太棒了！搞定了！激动的心情难以言表！第一次完整体验了基于Cloudflare的现代互联网开发，亲手配置R2对象存储、D1数据库和TOTP两步验证，感受到了前所未有的乐趣。更令人赞叹的是，整个项目前后端代码竟由AI辅助生成，却如此优雅。不禁唏嘘，以往的Web技术已然落伍，这才是云原生开发的真正魅力！

我想，这条朋友圈大概会被淹没在大家的日常生活分享里吧——没关系，我是真的开心，纯粹地开心。

反观我现在手头做的一些与技术相关的工作——那些技术其实早就过气了，可我们仍乐此不疲地一遍又一遍“翻地刨坑”。每每想到这一点，我都忍不住叹口气，活像《浪浪山的小妖精》里的那只黄鼠狼——从一个话痨，硬生生憋成了一个不想说话的角色。

## “赛棍”

有些事情，当你把它们从生命的不同时刻摘出来重新打量，会觉得——真是好笑得很。

这事得从二十多年前说起。那时候在学生社团里，我负责技术方向。有一次我们讨论是否要把“ACM ICPC”这类编程竞赛引入学院，我的态度很明确：那种东西容易养出“赛棍”，对实际的工程能力帮助不大。我坚持要做的是人工智能、网络开发、智能多媒体——不是标新立异，而是真心相信，那些才是技术的正路。

现在回头看，我当年的判断，其实没错。

这些年，AI成了风口，前端后端成了显学，媒体智能化更是无所不在。那些曾经难以被说服的方向，终究还是一步步走进了主流视野，只是——没有带上我这个说话的人。

我是个安静的旁观者，一个没被写进历史注脚的小角色。那些技术翻腾出浪花时，我正与供应商斗智斗勇，以为自己赢下了一城，回头看，其实是输了。风起云涌，也吹不到我这片角落。时代终究来了，却没叫上我。

而我当年不屑一顾的比赛们，也没消失。它们换了马甲，成了课程、成了简历、成了升学加分项；甚至成了我为孩子准备“特长项目”时不得不重新面对的“老朋友”；也成了我参加比赛时，又爱又怕的变量。

到底什么才算对？我曾以为我知道，现在不那么肯定了。有些方向的确是对的，但走得慢；有些路径看似狭窄，却活得顽强。你以为决定胜负的是选择，其实往往是时间、结构、甚至运气。你以为努力的尽头是舞台，可能只是下一个功能模块的PR。

可即便如此，我还是庆幸：我曾看见过那些火还未点燃的地方，并愿意在黑暗里摸索一会儿。

哪怕走出来的时候，灯已经被别人点亮，名字也写的是别人的。

## 没卡没感情

在黄仁勋的带领下，英伟达如今所向披靡，成为全球市值最高的企业。但如果时间倒回二十年前，它也不过是在显卡大战中略占上风的一方——即便赢了，也难免面临破产的风险。由此可见，“赢”从来不是一件简单的事。

我还记得，自己毕业离开广州之后，当年的小伙伴们就开始做 CUDA 了，我其实是看好他们的。只不过，他们最终没能坚持下来，后来选择了创业，一度做得很有名，我也真心为他们感到高兴。

回到这本书的内容，前三分之二读得颇为轻松，后面的三分之一则有些吃力。毕竟前面讲到的很多工作，我曾经亲手做过，也曾迷惑、困惑过。

我相信，这个领域中还有许多像我一样的研究者，默默无闻，也曾陷入同样的迷茫。不同的是，他们没有转身离开，而是选择迎难而上，一点一点地推动这个看似“不可能”的任务往前走。

至于后面的章节，若想真正领会程序构建的精妙，恐怕还得借助一块好点的显卡，跑一跑代码，开一次单步调试——那时你会发现，那些晦涩的字句，其实都藏着一种动手后的通透与理解。

竟然，我还没舍得买一台好点的工作站和显卡，现在用的还是第一代的AirPods。所谓的执着和坚持，最后都只是让自己开心一点，悲喜也难以与人共情，这恐怕是需要自己反思的。

看对了，押对了，但是人还是一样的拮据，这肯定不是对的，不然和阿Q有什么区别？

## 浮点数的情结

这十几年来，有些事像刺一样扎在心里，怎么也放不下来，其中一个就是关于混沌和人工智能是否能融合？我一直热爱着自己的专业，多年前我也是一名傲娇的研究生。自己的专业领域太窄，而自己也没法从更高的角度来思考这种专业性的融合事情，后来去找了学术上也有一定造诣的朋友，看是否有结合的地方。当时是，同样傲娇的朋友说了一句，你那个没用，这仿佛就是一根刺一样扎入了心里。

### 扎心

于是我拼命地寻找结合点，怎么可能没用呢？越是寻找，发现自己知道的就越少，于是很花了很多时间补充了一些背景的知识。直到看完这本书，我找到了结合点，后来又发了一篇朋友圈：

> **“浮点精度这只数值界的‘蝴蝶’，一头扇得确定性混沌在计算机里失真退化，另一头又把本来乖巧的动力学扇进数值混沌；如今，它也在牵着大模型的‘聪明劲儿’跑偏。”**

我知道，大部分朋友看不懂，匆匆地划过了。但是，我还是高兴，和以往一样的高兴。因为我在研究混沌密码学的时候，发现浮点数这个东西是万恶之源，是它的精度导致了蝴蝶效应（细微差异导致了巨大差异），是它的精度导致了混沌的动力学退化（就是算着算着就不混沌了），是它导致了我用不同编程语言来实现同一个模型得不到同样的数值，几次差点将我的密码分析工作引入歧途。毕竟大部分的密码学工作都是基于整型数的，只有极少数的才是基于浮点数的，所以才让我深刻地理解了浮点数的IEEE754标准，现在想想原本可能也有一个前景美好的科研方向，可惜自己放弃了前行。

原本我只想作为一点的小惊喜，自己开心了就好，只不过昨天Deepseek发布了V3.1，还专业说自己选择了UE8M0，准备匹配国产化的算力；一个星期前，OpenAI发布了自己的开源大模型GPTOSS，说自己用的是MXFP4；此前Deepseek R1也说自己用的是FP8。接着，一大堆的文档出现，说量化的大模型智力明显下降，本质上这些事情和我曾经研究过的IEEE754的浮点数是相关的。此外，深度学习中的Transformer算法，就是一个复杂一点的动力学系统，因为每次正向计算，反向梯度传播，就是将上一次的计算值带入下一次计算的计算范式，这就是明确的动力学系统。

孔子说，人不知而不愠，圣人能做到的事情，我是做不到的。我愠了，且耿耿于怀了许久。

### 异同

为了试图把我理解的事情讲清楚，我还是借助了大模型的力量。

想象大模型的世界就是个快递帝国。训练时要把海量包裹送进大仓库（显存），上线推理时还得把货送到全国各地（用户设备）。问题是：

包裹（参数/激活）多得像双十一，邮袋和车道（显存和带宽）有限，邮差（算力）天天加班。再用传统 FP16，邮差早猝死；于是行业发明了 **MX**，全称 **Microscaling（微缩放）**：**“分箱+共享比例尺”**。

每32个包裹一箱，箱子配一张 UE8M0 “比例尺地图”（2 的幂表示），包裹上只贴简码，到了再按地图放大，还原出大致重量和地址。故事的主角是三大快递村：

- **FP8村（DeepSeek R1）**：

  村民每封包裹都写完整地址，楼号、门牌、猫爬架都标清（指数+尾数），邮差送得很准。但缺点是：**一车装不了几单，邮费爆表**，邮差天天哭喊“我太难了”。

- **MXFP4村（gptoss）**：

  走极简路线，包裹只写4位缩写，整箱靠那张共享比例尺地图找路。好处：**一车塞满，跑得飞快，推理并发超多**；坏处：偶尔细节模糊，邮差遇到重名街道还得靠猜，笑称“快递盲盒”。

- **UE8M0 FP8村（DeepSeek v3.1）**：

  学精了，依然用微缩放，但包裹标签更聪明：只写“楼层号”（指数，无尾数）+共享地图。**范围特别大，跑长途不迷路，动态稳定**，训练稳、推理省，连乡下偏远边缘设备也能准时送达。邮差乐开花：“终于能双休了！”

如果没有 MX，这些邮差早在仓库里累趴，模型还没学会写诗，人先猝死。

有了 **Microscaling 微缩放**，**训练**像春运也不卡车，**推理**像双十一也能秒发货；显存省、带宽省、电费省，环保部门都点赞。

一句话：**MX 就是大模型的“快递省心术”，三村各有套路，但目标都是让包裹更轻快、准时又省钱。**

### 未来

从浮点数这个工程方面出发，我个人觉得，大模型往后还会出现两种训练的方向，一种是不断增加精度，感受混沌的力量带来的智能涌现，第二种是在现有的算法中调整精度，减少量化对模型带来的“智能下降”以及伴随而来的口水大战。

前者是面对未来的科学探索，后者是让现有的大模型能以算力更小的方式存在，既能减少开发厂家的算力耗费，又能提升普通计算设备的智能水平。随之而来的整个生态的调整，硬件首先得支持并能提供优化能力，中间件必须能最大限度地将前端的算力做好调度和调整，提升硬件的负载能力，最前端的框架必须能有良好的用户交互并将所谓的大模型计算任务显式地彰显出来。

# 结语

娃在考试，而我在旁边的星巴克碎碎念。

看着进进出出的年轻人，我发现自己真的老了，而且活得还没有年轻人潇洒。
